---
title: "Traffic Violations"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
library(tidyverse)
```

Traffic violations - from Cerda et al 2018
Traffic information from electronic violations issued in the Montgomery County of Maryland. 
Sample size (random subsample): 100,000. 
Target variable (multiclass-clf): ‘Violation type’ (4 classes). 
Selected categorical variable: ‘Description’ (card.: 3043). 
Other explanatory variables: ‘Belts’ (c), ‘Property Damage’ (c), ‘Fatal’ (c), ‘Commercial license’ (c), ‘Hazardous materials’ (c), ‘Commercial vehicle’ (c), ‘Alcohol’ (c), ‘Work zone’ (c), ‘Year’ (n), ‘Race’ (c), ‘Gender’ (c), ‘Arrest type’ (c).

Try tidymodels method on traffic violation data and compare distance method vs pco method

## Load data

```{r}
traffic_raw <- readr::read_csv("https://data.montgomerycountymd.gov/api/views/4mse-ku6q/rows.csv?accessType=DOWNLOAD") |>
  janitor::clean_names()
traffic_mapdat <- traffic_raw |> select(seq_id, location, latitude, longitude, geolocation, violation_type)
traffic_df <- traffic_raw |> 
  select(seq_id, violation_type, description, belts, property_damage, fatal, commercial_license, hazmat, commercial_vehicle, alcohol, work_zone, year, race, gender, arrest_type)
str(traffic_df)
```

data preprocessing following Cerda 2018

```{r}
traffic <- traffic_df |> 
  # remove rows with missing values for the target variable census_region (which brings number of classes down to 9 though???)
  filter(!is.na(`Violation Type`)) |> 
  # remove na from categorical_variable to avoid na in distance matrix
  # need to change to string (rather than "") so can use tidymodels with probability forests
  mutate(across(categorical_variable, \(.) {replace(.,is.na(.),"nan")})) |> 
  # remove rows with missing values for explanatory variables other than the selected categorical variable
  drop_na(where(is.character)) |> 
  # transform all entries for the categorical variable to lower case
  mutate(categorical_variable = tolower(categorical_variable)) |> 
  # convert to factors
  mutate(across(c(id, respondent, where(is.character)), factor))
str(midwest)

```


#check for na
traffic_df |> map(function(x) x |> is.na() |> table())

traffic_d <- traffic_df
traffic_pco <- traffic_df

load(file="../CAP_data/data/traffic_d.Rdata")
traffic_d |> as_tibble()

load(file="../CAP_data/data/traffic_pco.Rdata")
traffic_pco |> as_tibble()


## Build models

# Choose data
Method A is the distance method of Cerda. The 'story' variable has been split into 2778 distance variables
Method B is our pco method. The 'story' variable has undergone feature reduction and now only has 34 pco variables (to capture 95% of the variation)

# Split the data

For a probability forest to work, we can't have blank cells

If there are blanks, then need to convert "" to na, then convert to character, then replace na with "-" (or something different), then convert back to factor

```{r, include=FALSE}
#map_df(dat.d_for_tidymodels, \(x) which(x=="") |> length()) |> t() |> as.data.frame() |> filter(V1 !=0) # check for blanks
#map_df(dat.pco_for_tidymodels, \(x) which(x=="") |> length()) |> t() |> as.data.frame() |> filter(V1 !=0) # check for blanks
```

There are no blanks :-)

```{r}
library(tidymodels)

set.seed(123)
data_split.d <- initial_split(dat.d_for_tidymodels, prop = 0.8)
train_data.d <- training(data_split.d)
test_data.d  <- testing(data_split.d)

set.seed(123)
data_split.pco <- initial_split(dat.pco_for_tidymodels, prop = 0.8)
train_data.pco <- training(data_split.pco)
test_data.pco  <- testing(data_split.pco)

```



# Define the random forest model

A ranger probability forest is always fit using tidymodels unless the probability argument is changed via set_engine. I will leave it as default for now
The default respect.unordered.factors parameter is NULL (aka FALSE or "ignore") so I will change it to TRUE (aka "order") although this isn't strictly necessary when factors are converted to binary

```{r}
rf_mod <- 
  rand_forest(trees = 1000)  |>  
  set_engine("ranger", respect.unordered.factors = TRUE) |> 
  set_mode("classification")

```

# Preprocess the data (create a recipe)

Specify any variables which are not predictors (update_role)
One-hot encode all nominal predictors (step_dummy)
Check for singular dummy columns (step_zv)

```{r}
rf_recipe.d <- recipe(census_region ~ ., data = train_data.d) |> 
  update_role(id, respondent, new_role = "ID") |> 
  step_dummy(all_nominal_predictors(),one_hot=TRUE) |> 
  step_zv(all_predictors()) 

summary(rf_recipe.d)

rf_recipe.pco <- recipe(census_region ~ ., data = train_data.pco) |> 
  update_role(id, respondent, new_role = "ID") |> 
  step_dummy(all_nominal_predictors(),one_hot=TRUE) |> 
  step_zv(all_predictors()) 

summary(rf_recipe.pco)
```

Before using `prep()` the recipe steps have been defined but not actually run or implemented. The `prep()` function is where everything gets evaluated. You can use `juice()` to get the preprocessed data back out and check on your results.

```{r}
rf_prep.d <- prep(rf_recipe.d)
rf_juiced.d <- juice(rf_prep.d)

rf_prep.pco <- prep(rf_recipe.pco)
rf_juiced.pco <- juice(rf_prep.pco)
```


# Define workflow (bundle the model and recipe)

```{r}
rf_workflow.d <- workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe.d)

rf_workflow.d

rf_workflow.pco <- workflow() |> 
  add_model(rf_mod) |> 
  add_recipe(rf_recipe.pco)

rf_workflow.pco
```




# Train the model

```{r}
doParallel::registerDoParallel()
rf_fit.d <- rf_workflow.d |> 
  fit(train_data.d) 

doParallel::registerDoParallel()
rf_fit.pco <- rf_workflow.pco |> 
  fit(train_data.pco) 
```

Some models (but not ranger) have a tidy() method that provides the summary results in a more useful format `rf_fit |> tidy()`.



## Evaluate model

# Predict test data 

augment has more details, but only works with probability forests
```{r}
rf_predict.d <- predict(rf_fit.d, test_data.d)
rf_predict.d

rf_augment.d <- augment(rf_fit.d, test_data.d)
rf_augment.d

rf_predict.pco <- predict(rf_fit.pco, test_data.pco)
rf_predict.pco

rf_augment.pco <- augment(rf_fit.pco, test_data.pco)
rf_augment.pco
```


# Accuracy from testing data

How did this model do overall?
`type = "prob"` only works when probability forest is set to TRUE

```{r}
rf_testing_pred.d <- 
  rf_predict.d |> 
  bind_cols(predict(rf_fit.d, test_data.d, type = "prob")) |>  
  bind_cols(test_data.d |> select(census_region))

rf_testing_pred.d

rf_testing_pred.d |>                   
  roc_auc(truth = census_region, ".pred_East North Central":".pred_West South Central")

rf_testing_pred.d |>                  
  accuracy(truth = census_region, .pred_class)

rf_testing_pred.pco <- 
  rf_predict.pco |> 
  bind_cols(predict(rf_fit.pco, test_data.pco, type = "prob")) |>  
  bind_cols(test_data.pco |> select(census_region))

rf_testing_pred.pco

rf_testing_pred.pco |>                   
  roc_auc(truth = census_region, ".pred_East North Central":".pred_West South Central")

rf_testing_pred.pco |>                  
  accuracy(truth = census_region, .pred_class)

```

The accuracy is not great. 
ROC is adjusted for multiclass method (`hand-till` method).

There is an error at this line "Returning more (or less) than 1 row per `summarise()` group was deprecated in dplyr 1.1.0.
ℹ Please use `reframe()` instead"

```{r}
rf_testing_pred.d %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  autoplot()

rf_testing_pred.pco %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  autoplot()
```


# Accuracy via cross validation folds

- create training and testing sets
- create resampling folds from the *training* set

```{r}
set.seed(234)
#cv_folds.d <- vfold_cv(train_data.d, v = 10)
cv_folds.d <- vfold_cv(train_data.d, strata = census_region)

set.seed(234)
#cv_folds.pco <- vfold_cv(train_data.pco, v = 10)
cv_folds.pco <- vfold_cv(train_data.pco, strata = census_region)
```

```{r}
doParallel::registerDoParallel()
contrl_preds <- control_resamples(save_pred = TRUE)

rf_fit_cv.d <- fit_resamples(
  rf_workflow.d,
  resamples = cv_folds.d,
  control = contrl_preds
)

rf_fit_cv.pco <- fit_resamples(
  rf_workflow.pco,
  resamples = cv_folds.pco,
  control = contrl_preds
)
```

How did this model do overall?

```{r}
rf_fit_cv.d %>%
  collect_metrics()

rf_fit_cv.pco %>%
  collect_metrics()
```

We can create a confusion matrix to see how the different classes did.

```{r}
rf_fit_cv.d %>%
  collect_predictions() %>%
  conf_mat(census_region, .pred_class)

rf_fit_cv.pco %>%
  collect_predictions() %>%
  conf_mat(census_region, .pred_class)
```

```{r}
rf_fit_cv.d %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class)

rf_fit_cv.pco %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class)
```

```{r}
rf_fit_cv.d %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class) %>%
  ggplot(aes(.estimate)) + geom_histogram()

rf_fit_cv.pco %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class) %>%
  ggplot(aes(.estimate)) + geom_histogram()
```





# Bootstrap method

But should we fit the random forest model to bootstrap resamples instead?

```{r}
set.seed(345)
boot_folds.d <- dat.d_for_tidymodels |> 
  bootstraps()

set.seed(345)
boot_folds.pco <- dat.pco_for_tidymodels |> 
  bootstraps()
```

```{r}
doParallel::registerDoParallel()
rf_fit_boot.d <- fit_resamples(
  rf_workflow.d,
  resamples = boot_folds.d,
  control = control_resamples(save_pred = TRUE)
)

doParallel::registerDoParallel()
rf_fit_boot.pco <- fit_resamples(
  rf_workflow.pco,
  resamples = boot_folds.pco,
  control = control_resamples(save_pred = TRUE)
)

```

How did this model do overall?

```{r}
rf_fit_boot.d |> 
  collect_metrics()

rf_fit_boot.pco |> 
  collect_metrics()
```

The accuracy is not great. ROC is adjusted for multiclass method.

Since we saved the predictions with save_pred = TRUE we can compute other performance metrics. Notice that by default the positive predictive value (like accuracy) is macro-weighted for multiclass problems.

```{r}
rf_preds.d <- rf_fit_boot.d %>%
  collect_predictions()
rf_preds.d

rf_preds.d %>%
  accuracy(census_region, .pred_class)

rf_preds.d %>%
  roc_auc(truth=census_region, ".pred_East North Central":".pred_West South Central", estimator="hand_till")

rf_preds.d %>%
  ppv(census_region, .pred_class)


rf_preds.pco <- rf_fit_boot.pco %>%
  collect_predictions()
rf_preds.pco

rf_preds.pco %>%
  accuracy(census_region, .pred_class)

rf_preds.pco %>%
  roc_auc(truth=census_region, ".pred_East North Central":".pred_West South Central", estimator="hand_till")

rf_preds.pco %>%
  ppv(census_region, .pred_class)

```

How does it compare between resamples and how much does it vary

```{r}
rf_preds.d %>%
  group_by(id) %>%
  ppv(census_region, .pred_class)

rf_preds.pco %>%
  group_by(id) %>%
  ppv(census_region, .pred_class)

```

```{r}
rf_fit_boot.d %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class) %>%
  ggplot(aes(.estimate)) + geom_histogram()

rf_fit_boot.pco %>%
  collect_predictions() %>%
  group_by(id) %>%
  ppv(census_region, .pred_class) %>%
  ggplot(aes(.estimate)) + geom_histogram()
```

Next, let’s compute ROC curves for each class.

```{r}
rf_preds.d %>%
  group_by(id) %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  autoplot()

rf_preds.d %>%
  group_by(id) %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  ggplot(aes(1 - specificity, sensitivity, color = id)) +
  geom_abline(lty = 2, color = "gray80", linewidth = 1) +
  geom_path(show.legend = FALSE, alpha = 0.6, linewidth = 0.75) +
  facet_wrap(~.level, ncol = 4) +
  coord_equal() +
  theme_bw()


rf_preds.pco %>%
  group_by(id) %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  autoplot()

rf_preds.pco %>%
  group_by(id) %>%
  roc_curve(census_region, ".pred_East North Central":".pred_West South Central") %>%
  ggplot(aes(1 - specificity, sensitivity, color = id)) +
  geom_abline(lty = 2, color = "gray80", linewidth = 1) +
  geom_path(show.legend = FALSE, alpha = 0.6, linewidth = 0.75) +
  facet_wrap(~.level, ncol = 4) +
  coord_equal() +
  theme_bw()
```

We have an ROC curve for each class and each resample in this plot.

We can also compute a confusion matrix. 

```{r}
rf_preds.d %>%
  conf_mat(census_region, .pred_class)

rf_fit_boot.d |> tune::conf_mat_resampled()

rf_preds.pco %>%
  conf_mat(census_region, .pred_class)

rf_fit_boot.pco |> tune::conf_mat_resampled()

```



These counts are can be easier to understand in a visualization.

```{r}
#rf_preds.d %>%
#  conf_mat(census_region, .pred_class) %>%
#  autoplot(type = "heatmap")

(rf_preds.d %>%
  conf_mat(census_region, .pred_class))[[1]] %>%
  as.data.frame() %>%
  mutate(Truth = factor(Truth, levels=rev(levels(Truth)))) %>%
  ggplot(aes(x=Prediction, y=Truth)) + 
  geom_tile(aes(fill= Freq)) +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low="#eeeeee", high="orange") +
  theme_bw()


(rf_preds.pco %>%
  conf_mat(census_region, .pred_class))[[1]] %>%
  as.data.frame() %>%
  mutate(Truth = factor(Truth, levels=rev(levels(Truth)))) %>%
  ggplot(aes(x=Prediction, y=Truth)) + 
  geom_tile(aes(fill= Freq)) +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low="#eeeeee", high="blue") +
  theme_bw()


```

There is some real variability on the diagonal.

If we set the diagonal to all zeroes, we can see which classes were most likely to be confused for each other.

```{r}
(rf_preds.d %>%
  filter(.pred_class != census_region) %>%
  conf_mat(census_region, .pred_class))[[1]] %>%
  as.data.frame() %>%
  mutate(Truth = factor(Truth, levels=rev(levels(Truth)))) %>%
  ggplot(aes(x=Prediction, y=Truth)) + 
  geom_tile(aes(fill= Freq)) +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low="#eeeeee", high="orange") +
  theme_bw()


(rf_preds.pco %>%
  filter(.pred_class != census_region) %>%
  conf_mat(census_region, .pred_class))[[1]] %>%
  as.data.frame() %>%
  mutate(Truth = factor(Truth, levels=rev(levels(Truth)))) %>%
  ggplot(aes(x=Prediction, y=Truth)) + 
  geom_tile(aes(fill= Freq)) +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low="#eeeeee", high="blue") +
  theme_bw()

```

West `North Central` was confused with `East North Central`, and `Middle Atlantic` was confused with `South Atlantic`.




## Variable importance

What can we learn about variable importance, using the vip package?

### 1. ranger:permutation method

Update model

```{r}
library(vip)

vip_rf_mod <- rf_mod |> 
  set_engine("ranger", respect.unordered.factors = TRUE, importance = "permutation")

vip_rf_workflow.d <- 
  rf_workflow.d |> 
  update_model(vip_rf_mod)

vip_rf_workflow.pco <- 
  rf_workflow.pco |> 
  update_model(vip_rf_mod)
```

Fit model

```{r}
rf_fit.d <- vip_rf_workflow.d |> 
  fit(dat.d_for_tidymodels)

rf_fit.pco <- vip_rf_workflow.pco |> 
  fit(dat.pco_for_tidymodels)
```

Extract parsnip and run vi() and plot

```{r}
vip.d <- rf_fit.d |> 
  extract_fit_parsnip()
vip.d |> 
  vip(num_features = 30, geom = "point", aesthetics = list(color = "red")) + ggtitle("Cerda : permutation")
vip.d |> vi()


vip.pco <- rf_fit.pco |> 
  extract_fit_parsnip() 
vip.pco |> 
  vip(num_features = 30, geom = "point", aesthetics = list(color = "blue")) + ggtitle("PCO : permutation")

vip.pco |> vi()

```

### 2. ranger:impurity_corrected method

Update model

```{r}
vip_rf_mod <- rf_mod |> 
  set_engine("ranger", respect.unordered.factors = TRUE, importance = "impurity_corrected")

vip_rf_workflow.d <- 
  rf_workflow.d |> 
  update_model(vip_rf_mod)

vip_rf_workflow.pco <- 
  rf_workflow.pco |> 
  update_model(vip_rf_mod)
```

Fit model

```{r}
rf_fit.d <- vip_rf_workflow.d |> 
  fit(dat.d_for_tidymodels)

rf_fit.pco <- vip_rf_workflow.pco |> 
  fit(dat.pco_for_tidymodels)
```

Extract parsnip and run vi() and plot

```{r}
vip.d <- rf_fit.d |> 
  extract_fit_parsnip()
vip.d |> 
  vip(num_features = 30, geom = "point", aesthetics = list(color = "red")) + ggtitle("Cerda : impurity_corrected")
vip.d |> vi()


vip.pco <- rf_fit.pco |> 
  extract_fit_parsnip() 
vip.pco |> 
  vip(num_features = 30, geom = "point", aesthetics = list(color = "blue")) + ggtitle("PCO : impurity_corrected")

vip.pco |> vi()

```

### 3. classification tree with vip importance

Update model

```{r}
vip_rf_mod <- rf_mod |> 
  set_engine("ranger", respect.unordered.factors = TRUE, probability = FALSE)

vip_rf_workflow.d <- 
  rf_workflow.d |> 
  update_model(vip_rf_mod)

vip_rf_workflow.pco <- 
  rf_workflow.pco |> 
  update_model(vip_rf_mod)
```

Fit model

```{r}
rf_fit.d <- vip_rf_workflow.d |> 
  fit(dat.d_for_tidymodels)

rf_fit.pco <- vip_rf_workflow.pco |> 
  fit(dat.pco_for_tidymodels)
```

Run vi() using "permute" , repeat nsim times, and plot
Define pfun which pulls out the predicted classes .pred_Class

```{r}
pfun <- function(object, newdata) {
  predict(object, new_data = newdata, type = "class")$.pred_class
}

rf_fit.d |> 
  vip(method = "permute", target = "census_region", metric = "accuracy", 
      pred_wrapper = pfun, train = dat.d_for_tidymodels,
      nsim = 10, geom = "boxplot", all_permutations = TRUE, 
      mapping = aes(fill = !!sym("Variable"))) + 
  ggtitle("Midwest vip scores from classification forest using Cerda method", subtitle = "generated with VIP")

rf_fit.pco |> 
  vip(method = "permute", target = "census_region", metric = "accuracy", 
      pred_wrapper = pfun, train = dat.pco_for_tidymodels,
      nsim = 10, geom = "boxplot", all_permutations = TRUE, 
      mapping = aes(fill = !!sym("Variable"))) + 
  ggtitle("Midwest vip scores from classification forest using pco method", subtitle = "generated with VIP")

```


## Testing data

The function last_fit() *fits* one final time on the training data and *evaluates* on the testing data. This is the first time we have used the testing data.

```{r}
final_fitted.d <- last_fit(rf_workflow.d, data_split.d)
collect_metrics(final_fitted.d)  ## metrics evaluated on the *testing* data

final_fitted.pco <- last_fit(rf_workflow.pco, data_split.pco)
collect_metrics(final_fitted.pco)  ## metrics evaluated on the *testing* data
```

This object contains a fitted workflow that we can use for prediction.

```{r}
final_wf.d <- extract_workflow(final_fitted.d)
predict(final_wf.d, test_data.d)

final_wf.pco <- extract_workflow(final_fitted.pco)
predict(final_wf.pco, test_data.pco)
```























